# Deep Learning: Technology and Applications(Spring 2021)
Deep Learning is regarded as a panacea in the era of big data.
In this course, you will learn the foundations of deep learning, understand how to interprete others' neural networks, and learn how to build your own neural networks.
You will also learn about popular neural network structures including Convolutional neural networks, RNNs, LSTM, and transformers.

## Course Information
- This course meets for in-class lecture Fri 9:00AM - 12:00AM (Seminar room No.2 at KISTI KIUM).
- For all inquiries related to this course, please contact bart7449 AT gmail DOT com
### Instructor
- Kyong-Ha Lee 
### Time and Location
- Fri. 9:00 AM ~ 12:00PM
## Materials
- Book: Neural networks and deep learning: a textbook by Charu C. Aggarwal(but not mandatory)
- All slides for this class will be available here. 
- Some papers in a reading list will be reviewed and presented by students  
## Logistics
- All course announcements take place though this page. Please check this page frequently.
### Class components and grading
- This course has the following components:
  - In-class lecture (1.5~2H) (attendance 10%)
  - Paper/code reivew(0~1.5H)(30%)
  - A final exam(60%)

## Syllabus
|Event|Date|In-class lecture|Materials and Assignments|
|------|----|-------------|---------|
|Lecture 1|03/05|Topics: (slides)| No assignments 
- Notation for understanding deep learning models|

## Reading list for further discussion
### General Techniques
- [Ioffe15a] <a href="http://proceedings.mlr.press/v37/ioffe15.pdf"> Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>, In Proceedings of ICLR 2015.
- (optional) <a href="https://arxiv.org/pdf/1607.06450.pdf?utm_source=sciontist.com">Layer Normalization</a>, arXiv:1607.06450v1, 2016.
- [Hinton15a] <a href="https://arxiv.org/pdf/1503.02531.pdf">Distilling the Knowledge in a Neural Network</a>, arXiv:1503.02531v1, 2015.

### Convolutional Neural Network
- [Alex2012a] <a href="https://kr.nvidia.com/content/tesla/pdf/machine-learning/imagenet-classification-with-deep-convolutional-nn.pdf">ImageNet Classification with Deep Convolutional Neural Networks </a>,  Advances in Neural Informaiton Processing Systems 25:1098-1105, 2012
- [He16a] <a href="https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf">Deep Residual Learning for Image Recognition</a>, In Proceedings of CVPR 2016.
- [He16b] <a href="https://arxiv.org/pdf/1603.05027.pdf">identity mapping in deep residual networks</a>, In Proceedings of ECCV 2016.
- [Huang17a] <a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf">Densely connected convolutional networks</a>, In Proceedings of CVPR 2017.
- [Hu2018]<a href="">Squeeze-and-excitation networks</a>, In Proceedings of CVPR 2018.

### Distributed Representations for words
- [Mikolov13a] <a href="https://arxiv.org/pdf/1310.4546.pdf">Distributed Representations of Words and Phrases and their Compositionality</a>, Advances in Neural Information Processing Systems 26 (2013): 3111-3119.
- [Rong2016a] <a href="https://arxiv.org/pdf/1411.2738.pdf&xid=25657,15700021,15700124,15700149,15700168,15700186,15700191,15700201,15700208&usg=ALkJrhhNCZKc2CO7hRoTrGd6aH2nBc-ZVQ">word2vec Parameter Learning Explained</a>, arXiv:1411.2738v4
- [Le14a]<a href="http://proceedings.mlr.press/v32/le14.pdf">Distributed Representations of Sentences and Documents</a>, In Proceedings of the ICML 2014.
- [Bojanowski17a] <a href="https://www.mitpressjournals.org/doi/pdfplus/10.1162/tacl_a_00051?source=post_page---------------------------">Enriching word vectors with subword information</a>, Transactions of the Association for Computational Linguistics 5 (2017): 135-146.

### Attention and Transformers
- [Badahnau16a] <a href="https://arxiv.org/pdf/1409.0473.pdf"> NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE</a>, In Proceedings of ICLR 2015
- [Vaswani17a] <a href="https://arxiv.org/pdf/1706.03762.pdf">Attention is All You Need<a>, In Proceedings of NeurIPS 2017  
- [Devlin19a] <a href="https://www.aclweb.org/anthology/N19-1423/">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>, In Proceedings of ACL 2019

### Lightweight Model
  - [Rastegary2016a] <a href="https://arxiv.org/pdf/1603.05279.pdf?source=post_page---------------------------">XNOR-Net: ImageNet classification using binary convolutional Neural Networks</a>, In Proceedings of ECCV 2016
  - [Sandler2018a] <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.pdf">Mobilenetv2: Inverted residuals and linear bottlenecks</a>, In Proceedings of CVPR 2018
  - [Stock2020a] <a href="https://arxiv.org/pdf/1907.05686.pdf?utm_campaign=ExternalTechRecSeptember52019&utm_content=100275348&utm_medium=social&utm_source=twitter&hss_channel=tw-992153930095251456">And the bit goes down: revisiting the quantization of neural networks</a>, In Proceedings of ICLR 2020
