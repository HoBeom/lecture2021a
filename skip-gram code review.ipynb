{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **We will implement Word2Vec. (Skip-gram)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Build corpus from nltk brown.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\master\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 87004\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import itertools\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for cat in ['news']:\n",
    "    for text_id in brown.fileids(cat):\n",
    "        raw_text = list(itertools.chain.from_iterable(brown.sents(text_id)))\n",
    "        text = ' '.join(raw_text)\n",
    "        text = text.lower()\n",
    "        text.replace('\\n', ' ')\n",
    "        text = re.sub('[^a-z ]+', '', text)\n",
    "        corpus.append([w for w in text.split() if w != ''])\n",
    "temp=[]\n",
    "for i in corpus:\n",
    "  temp = temp+i\n",
    "print(len(corpus),len(temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Implement subsampling & Make vocabulary.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 66648\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import random, math\n",
    "\n",
    "def subsample_frequent_words(corpus):\n",
    "    filtered_corpus = []\n",
    "    word_counts = dict(Counter(list(itertools.chain.from_iterable(corpus))))\n",
    "    sum_word_counts = sum(list(word_counts.values()))\n",
    "    word_counts = {word: word_counts[word]/float(sum_word_counts) for word in word_counts}\n",
    "    for text in corpus:\n",
    "        filtered_corpus.append([])\n",
    "        for word in text:\n",
    "            if random.random() < (1-math.sqrt(1e-5/word_counts[word])):\n",
    "            #if random.random() < (1+math.sqrt(word_counts[word] * 1e3)) * 1e-3 / float(word_counts[word]):\n",
    "                filtered_corpus[-1].append(word)\n",
    "    return filtered_corpus\n",
    "\n",
    "corpus = subsample_frequent_words(corpus)\n",
    "temp=[]\n",
    "for i in corpus:\n",
    "      temp = temp+i\n",
    "print(len(corpus), len(temp))\n",
    "\n",
    "vocabulary = set(itertools.chain.from_iterable(corpus))\n",
    "word_to_index = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "index_to_word = {idx: w for (idx, w) in enumerate(vocabulary)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Building bag of words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 535096 pairs of target and context words\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "context_tuple_list = []\n",
    "w = 4\n",
    "\n",
    "for text in corpus:\n",
    "    for i, word in enumerate(text):\n",
    "        first_context_word_index = max(0,i-w)\n",
    "        last_context_word_index = min(i+w, len(text))\n",
    "        for j in range(first_context_word_index, last_context_word_index+1):\n",
    "        #for j in range(first_context_word_index, last_context_word_index):\n",
    "            if j == len(text):\n",
    "                  continue;\n",
    "            if i!=j:\n",
    "                  context_tuple_list.append((word, text[j]))\n",
    "            \n",
    "print(\"There are {} pairs of target and context words\".format(len(context_tuple_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Implement network.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4-1. CBOW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size, vocab_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.linear = nn.Linear(embedding_size, vocab_size)\n",
    "        \n",
    "    def forward(self, context_word):\n",
    "        emb = self.embeddings(context_word)\n",
    "        hidden = self.linear(emb)\n",
    "        out = F.log_softmax(hidden)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4-2. Skip-gram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Skip_gram(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size, vocab_size):\n",
    "        super(Skip_gram, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.linear = nn.Linear(embedding_size, vocab_size)\n",
    "        \n",
    "    def forward(self, target_word):\n",
    "        emb = self.embeddings(target_word)\n",
    "        hidden = self.linear(emb)\n",
    "        out = F.log_softmax(hidden)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. Training network.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5-1. Implement earlystopping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    def __init__(self, patience=5, min_percent_gain=0.1):\n",
    "        self.patience = patience\n",
    "        self.loss_list = []\n",
    "        self.min_percent_gain = min_percent_gain / 100.\n",
    "        \n",
    "    def update_loss(self, loss):\n",
    "        self.loss_list.append(loss)\n",
    "        if len(self.loss_list) > self.patience:\n",
    "            del self.loss_list[0]\n",
    "    \n",
    "    def stop_training(self):\n",
    "        if len(self.loss_list) == 1:\n",
    "            return False\n",
    "        gain = (max(self.loss_list) - min(self.loss_list)) / max(self.loss_list)\n",
    "        print(\"Loss gain: {}%\".format(round(100*gain,2)))\n",
    "        if gain < self.min_percent_gain:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5-2. Implemnet get batches function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_batches(context_tuple_list, batch_size=100):\n",
    "    random.shuffle(context_tuple_list)\n",
    "    batches = []\n",
    "    batch_target, batch_context = [], []\n",
    "    for i in range(len(context_tuple_list)):\n",
    "        batch_target.append(word_to_index[context_tuple_list[i][0]])\n",
    "        batch_context.append(word_to_index[context_tuple_list[i][1]])\n",
    "        if (i+1) % batch_size == 0 or i == len(context_tuple_list)-1:\n",
    "            tensor_target = autograd.Variable(torch.from_numpy(np.array(batch_target)).long())\n",
    "            tensor_context = autograd.Variable(torch.from_numpy(np.array(batch_context)).long())\n",
    "            batches.append((tensor_target, tensor_context))\n",
    "            batch_target, batch_context = [], []\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5-3. Training model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\master\\anaconda3\\envs\\ood_ex\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  8.667883766231252\n",
      "Loss:  8.274657875744264\n",
      "Loss gain: 4.54%\n",
      "Loss:  7.810480075096017\n",
      "Loss gain: 9.89%\n",
      "Loss:  7.298139027695157\n",
      "Loss gain: 15.8%\n",
      "Loss:  6.941162452768924\n",
      "Loss gain: 19.92%\n",
      "Loss:  6.727393842455166\n",
      "Loss gain: 18.7%\n",
      "Loss:  6.590473603846422\n",
      "Loss gain: 15.62%\n",
      "Loss:  6.498827443194034\n",
      "Loss gain: 10.95%\n",
      "Loss:  6.435112036875824\n",
      "Loss gain: 7.29%\n",
      "Loss:  6.390041886870541\n",
      "Loss gain: 5.01%\n",
      "Loss:  6.357535949393885\n",
      "Loss gain: 3.53%\n",
      "Loss:  6.333875874974835\n",
      "Loss gain: 2.54%\n",
      "Loss:  6.3160373915487265\n",
      "Loss gain: 1.85%\n",
      "Loss:  6.302103690247037\n",
      "Loss gain: 1.38%\n",
      "Loss:  6.291039975721445\n",
      "Loss gain: 1.05%\n",
      "Loss:  6.281961015800931\n",
      "Loss gain: 0.82%\n",
      "Loss:  6.274257272037108\n",
      "Loss gain: 0.66%\n",
      "Loss:  6.268058426344573\n",
      "Loss gain: 0.54%\n",
      "Loss:  6.262534856796265\n",
      "Loss gain: 0.45%\n",
      "Loss:  6.25797357843883\n",
      "Loss gain: 0.38%\n",
      "Loss:  6.253947149461775\n",
      "Loss gain: 0.32%\n",
      "Loss:  6.25068257637878\n",
      "Loss gain: 0.28%\n",
      "Loss:  6.2476917985659925\n",
      "Loss gain: 0.24%\n",
      "Loss:  6.2448978228355525\n",
      "Loss gain: 0.21%\n",
      "Loss:  6.242582874511605\n",
      "Loss gain: 0.18%\n",
      "Loss:  6.240564677252698\n",
      "Loss gain: 0.16%\n",
      "Loss:  6.238750608999338\n",
      "Loss gain: 0.14%\n",
      "Loss:  6.237020827051419\n",
      "Loss gain: 0.13%\n",
      "Loss:  6.235632428482397\n",
      "Loss gain: 0.11%\n",
      "Loss:  6.234184003587979\n",
      "Loss gain: 0.1%\n",
      "Loss:  6.2327751526192054\n",
      "Loss gain: 0.1%\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(vocabulary)\n",
    "\n",
    "net = Skip_gram(embedding_size=2, vocab_size=vocabulary_size)\n",
    "net= net.cuda()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "early_stopping = EarlyStopping()\n",
    "\n",
    "context_tensor_list = []\n",
    "\n",
    "context_tensor_list = []\n",
    "\n",
    "for target, context in context_tuple_list:\n",
    "    target_tensor = autograd.Variable(torch.LongTensor([word_to_index[target]]))\n",
    "    context_tensor = autograd.Variable(torch.LongTensor([word_to_index[context]]))\n",
    "    context_tensor_list.append((target_tensor, context_tensor))\n",
    "\n",
    "while True:\n",
    "    losses = []\n",
    "    context_tuple_batches = get_batches(context_tuple_list, batch_size=2000)\n",
    "    for target_tensor, context_tensor in context_tuple_batches:\n",
    "        target_tensor=target_tensor.cuda()\n",
    "        context_tensor=context_tensor.cuda()\n",
    "        net.zero_grad()\n",
    "        log_probs = net(target_tensor)\n",
    "        loss = loss_function(log_probs, context_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    print(\"Loss: \", np.mean(losses))\n",
    "    early_stopping.update_loss(np.mean(losses))\n",
    "    if early_stopping.stop_training():\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **6. Check original skip-gram result.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pedestal', 0.008338884450495243),\n",
       " ('date', 0.010459519922733307),\n",
       " ('impressive', 0.019094765186309814),\n",
       " ('enforcement', 0.019178779795765877),\n",
       " ('hearing', 0.019241103902459145)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_closest_word(word, topn=5):\n",
    "    word_distance = []\n",
    "    emb = net.embeddings\n",
    "    pdist = nn.PairwiseDistance()\n",
    "    i = word_to_index[word]\n",
    "    lookup_tensor_i = torch.tensor([i], dtype=torch.long)\n",
    "    v_i = emb(lookup_tensor_i)\n",
    "    for j in range(len(vocabulary)):\n",
    "        if j != i:\n",
    "            lookup_tensor_j = torch.tensor([j], dtype=torch.long)\n",
    "            v_j = emb(lookup_tensor_j)\n",
    "            word_distance.append((index_to_word[j], float(pdist(v_i, v_j))))\n",
    "    word_distance.sort(key=lambda x: x[1])\n",
    "    return word_distance[:topn]\n",
    "net=net.cpu()\n",
    "get_closest_word(\"air\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **7. Implement skip-gram with negative sampling.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(context_tuple_list, batch_size=100):\n",
    "    random.shuffle(context_tuple_list)\n",
    "    batches = []\n",
    "    batch_target, batch_context, batch_negative = [], [], []\n",
    "    for i in range(len(context_tuple_list)):\n",
    "        batch_target.append(word_to_index[context_tuple_list[i][0]])\n",
    "        batch_context.append(word_to_index[context_tuple_list[i][1]])\n",
    "        batch_negative.append([word_to_index[w] for w in context_tuple_list[i][2]])\n",
    "        if (i+1) % batch_size == 0 or i == len(context_tuple_list)-1:\n",
    "            tensor_target = autograd.Variable(torch.from_numpy(np.array(batch_target)).long())\n",
    "            tensor_context = autograd.Variable(torch.from_numpy(np.array(batch_context)).long())\n",
    "            tensor_negative = autograd.Variable(torch.from_numpy(np.array(batch_negative)).long())\n",
    "            batches.append((tensor_target, tensor_context, tensor_negative))\n",
    "            batch_target, batch_context, batch_negative = [], [], []\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import multinomial\n",
    "\n",
    "def sample_negative(sample_size):\n",
    "    sample_probability = {}\n",
    "    word_counts = dict(Counter(list(itertools.chain.from_iterable(corpus))))\n",
    "    normalizing_factor = sum([v**0.75 for v in word_counts.values()])\n",
    "    for word in word_counts:\n",
    "        sample_probability[word] = word_counts[word]**0.75 / normalizing_factor\n",
    "    words = np.array(list(word_counts.keys()))\n",
    "    while True:\n",
    "        word_list = []\n",
    "        sampled_index = np.array(multinomial(sample_size, list(sample_probability.values())))\n",
    "        for index, count in enumerate(sampled_index):\n",
    "            for _ in range(count):\n",
    "                 word_list.append(words[index])\n",
    "        yield word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "context_tuple_list = []\n",
    "w = 4\n",
    "negative_samples = sample_negative(8)\n",
    "\n",
    "for text in corpus:\n",
    "    for i, word in enumerate(text):\n",
    "        first_context_word_index = max(0,i-w)\n",
    "        last_context_word_index = min(i+w, len(text))\n",
    "        for j in range(first_context_word_index, last_context_word_index):\n",
    "            if i!=j:\n",
    "                context_tuple_list.append((word, text[j], next(negative_samples)))\n",
    "print(\"There are {} pairs of target and context words\".format(len(context_tuple_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('county',\n",
       " 'jury',\n",
       " ['in', 'and', 'given', 'during', 'instead', 'north', 'silk', 'bail'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_tuple_list[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Word2Vec(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size, vocab_size):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.embeddings_target = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.embeddings_context = nn.Embedding(vocab_size, embedding_size)\n",
    "\n",
    "    def forward(self, target_word, context_word, negative_example):\n",
    "        emb_target = self.embeddings_target(target_word)\n",
    "        emb_context = self.embeddings_context(context_word)\n",
    "        emb_product = torch.mul(emb_target, emb_context)\n",
    "        emb_product = torch.sum(emb_product, dim=1)\n",
    "        out = torch.sum(F.logsigmoid(emb_product))\n",
    "        emb_negative = self.embeddings_context(negative_example)\n",
    "        emb_product = torch.bmm(emb_negative, emb_target.unsqueeze(2))\n",
    "        emb_product = torch.sum(emb_product, dim=1)\n",
    "        out += torch.sum(F.logsigmoid(-emb_product))\n",
    "        return -out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  37674.032081117024\n",
      "Loss:  29828.38278964428\n",
      "Loss gain: 20.83%\n",
      "Loss:  24712.928187333775\n",
      "Loss gain: 34.4%\n",
      "Loss:  20838.180414727394\n",
      "Loss gain: 44.69%\n",
      "Loss:  17793.955802235705\n",
      "Loss gain: 52.77%\n",
      "Loss:  15349.836066842587\n",
      "Loss gain: 48.54%\n",
      "Loss:  13357.566818691821\n",
      "Loss gain: 45.95%\n",
      "Loss:  11718.488663044382\n",
      "Loss gain: 43.76%\n",
      "Loss:  10358.081783992686\n",
      "Loss gain: 41.79%\n",
      "Loss:  9217.57806370512\n",
      "Loss gain: 39.95%\n",
      "Loss:  8249.668448200631\n",
      "Loss gain: 38.24%\n",
      "Loss:  7420.793245615858\n",
      "Loss gain: 36.67%\n",
      "Loss:  6704.4787696351395\n",
      "Loss gain: 35.27%\n",
      "Loss:  6079.666338202293\n",
      "Loss gain: 34.04%\n",
      "Loss:  5531.267810058594\n",
      "Loss gain: 32.95%\n",
      "Loss:  5047.107274871177\n",
      "Loss gain: 31.99%\n",
      "Loss:  4618.132390967836\n",
      "Loss gain: 31.12%\n",
      "Loss:  4235.805659387467\n",
      "Loss gain: 30.33%\n",
      "Loss:  3894.0056012092755\n",
      "Loss gain: 29.6%\n",
      "Loss:  3587.2438827189994\n",
      "Loss gain: 28.92%\n",
      "Loss:  3311.2305193961934\n",
      "Loss gain: 28.3%\n",
      "Loss:  3062.476220703125\n",
      "Loss gain: 27.7%\n",
      "Loss:  2837.128860798288\n",
      "Loss gain: 27.14%\n",
      "Loss:  2632.414570260555\n",
      "Loss gain: 26.62%\n",
      "Loss:  2446.211354357131\n",
      "Loss gain: 26.12%\n",
      "Loss:  2276.6485006129487\n",
      "Loss gain: 25.66%\n",
      "Loss:  2122.4439412218458\n",
      "Loss gain: 25.19%\n",
      "Loss:  1981.641777655419\n",
      "Loss gain: 24.72%\n",
      "Loss:  1853.1736089178855\n",
      "Loss gain: 24.24%\n",
      "Loss:  1736.232500311669\n",
      "Loss gain: 23.74%\n",
      "Loss:  1629.7101502763464\n",
      "Loss gain: 23.22%\n",
      "Loss:  1532.4784506290516\n",
      "Loss gain: 22.67%\n",
      "Loss:  1444.214595258997\n",
      "Loss gain: 22.07%\n",
      "Loss:  1363.873355037608\n",
      "Loss gain: 21.45%\n",
      "Loss:  1290.8091865701879\n",
      "Loss gain: 20.8%\n",
      "Loss:  1224.644572481196\n",
      "Loss gain: 20.09%\n",
      "Loss:  1164.6364748690992\n",
      "Loss gain: 19.36%\n",
      "Loss:  1110.347884677319\n",
      "Loss gain: 18.59%\n",
      "Loss:  1061.2440249179272\n",
      "Loss gain: 17.78%\n",
      "Loss:  1016.5363868226397\n",
      "Loss gain: 16.99%\n",
      "Loss:  976.0930060204039\n",
      "Loss gain: 16.19%\n",
      "Loss:  939.309078330182\n",
      "Loss gain: 15.4%\n",
      "Loss:  905.8407991449884\n",
      "Loss gain: 14.64%\n",
      "Loss:  875.5046604886968\n",
      "Loss gain: 13.87%\n",
      "Loss:  847.7911639274434\n",
      "Loss gain: 13.14%\n",
      "Loss:  822.6742279377389\n",
      "Loss gain: 12.42%\n",
      "Loss:  799.770684424867\n",
      "Loss gain: 11.71%\n",
      "Loss:  778.7497867990047\n",
      "Loss gain: 11.05%\n",
      "Loss:  759.7092594877203\n",
      "Loss gain: 10.39%\n",
      "Loss:  742.2884938341506\n",
      "Loss gain: 9.77%\n",
      "Loss:  726.2641330475503\n",
      "Loss gain: 9.19%\n",
      "Loss:  711.3882302791515\n",
      "Loss gain: 8.65%\n",
      "Loss:  697.7248315364757\n",
      "Loss gain: 8.16%\n",
      "Loss:  685.4204924887799\n",
      "Loss gain: 7.66%\n",
      "Loss:  673.6923409969248\n",
      "Loss gain: 7.24%\n",
      "Loss:  662.9200043049265\n",
      "Loss gain: 6.81%\n",
      "Loss:  653.0463540259829\n",
      "Loss gain: 6.4%\n",
      "Loss:  643.7654043644033\n",
      "Loss gain: 6.08%\n",
      "Loss:  635.058682023718\n",
      "Loss gain: 5.73%\n",
      "Loss:  626.9519691629613\n",
      "Loss gain: 5.43%\n",
      "Loss:  619.4572920454309\n",
      "Loss gain: 5.14%\n",
      "Loss:  612.2321583200008\n",
      "Loss gain: 4.9%\n",
      "Loss:  605.5805405961706\n",
      "Loss gain: 4.64%\n",
      "Loss:  599.2842192954206\n",
      "Loss gain: 4.41%\n",
      "Loss:  593.325266866481\n",
      "Loss gain: 4.22%\n",
      "Loss:  587.9843925313746\n",
      "Loss gain: 3.96%\n",
      "Loss:  582.5616301516269\n",
      "Loss gain: 3.8%\n",
      "Loss:  577.7151872350814\n",
      "Loss gain: 3.6%\n",
      "Loss:  572.6981089490525\n",
      "Loss gain: 3.48%\n",
      "Loss:  568.5303292781749\n",
      "Loss gain: 3.31%\n",
      "Loss:  564.1485160178327\n",
      "Loss gain: 3.16%\n",
      "Loss:  560.032466807264\n",
      "Loss gain: 3.06%\n",
      "Loss:  556.2994926614964\n",
      "Loss gain: 2.86%\n",
      "Loss:  552.5250876244078\n",
      "Loss gain: 2.82%\n",
      "Loss:  549.1851367706948\n",
      "Loss gain: 2.65%\n",
      "Loss:  545.7198624306536\n",
      "Loss gain: 2.56%\n",
      "Loss:  542.612152651523\n",
      "Loss gain: 2.46%\n",
      "Loss:  539.4758119623712\n",
      "Loss gain: 2.36%\n",
      "Loss:  536.7141571044922\n",
      "Loss gain: 2.27%\n",
      "Loss:  533.840174313809\n",
      "Loss gain: 2.18%\n",
      "Loss:  531.2802312323388\n",
      "Loss gain: 2.09%\n",
      "Loss:  528.8052186032559\n",
      "Loss gain: 1.98%\n",
      "Loss:  526.6123927177267\n",
      "Loss gain: 1.88%\n",
      "Loss:  524.0847258222864\n",
      "Loss gain: 1.83%\n",
      "Loss:  522.0008137804397\n",
      "Loss gain: 1.75%\n",
      "Loss:  519.9338991692725\n",
      "Loss gain: 1.68%\n",
      "Loss:  517.9994451969228\n",
      "Loss gain: 1.64%\n",
      "Loss:  516.0535203000333\n",
      "Loss gain: 1.53%\n",
      "Loss:  514.2564360922955\n",
      "Loss gain: 1.48%\n",
      "Loss:  512.5687171449052\n",
      "Loss gain: 1.42%\n",
      "Loss:  511.0229374662359\n",
      "Loss gain: 1.35%\n",
      "Loss:  509.4168516930113\n",
      "Loss gain: 1.29%\n",
      "Loss:  507.8507749841568\n",
      "Loss gain: 1.25%\n",
      "Loss:  506.55034098523726\n",
      "Loss gain: 1.17%\n",
      "Loss:  505.2177400304916\n",
      "Loss gain: 1.14%\n",
      "Loss:  503.8126160317279\n",
      "Loss gain: 1.1%\n",
      "Loss:  502.5058568751558\n",
      "Loss gain: 1.05%\n",
      "Loss:  501.2784802700611\n",
      "Loss gain: 1.04%\n",
      "Loss:  500.07058014565325\n",
      "Loss gain: 1.02%\n",
      "Loss:  499.09040699410946\n",
      "Loss gain: 0.94%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "\n",
    "#loss_function = nn.CrossEntropyLoss()\n",
    "net = Word2Vec(embedding_size=200, vocab_size=vocabulary_size)\n",
    "net=net.cuda()\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "early_stopping = EarlyStopping(patience=5, min_percent_gain=1)\n",
    "\n",
    "while True:\n",
    "    losses = []\n",
    "    context_tuple_batches = get_batches(context_tuple_list, batch_size=2000)\n",
    "    for i in range(len(context_tuple_batches)):\n",
    "        net.zero_grad()\n",
    "        target_tensor, context_tensor, negative_tensor = context_tuple_batches[i]\n",
    "        target_tensor, context_tensor, negative_tensor = target_tensor.cuda(), context_tensor.cuda(), negative_tensor.cuda()\n",
    "        loss = net(target_tensor, context_tensor, negative_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    print(\"Loss: \", np.mean(losses))\n",
    "    early_stopping.update_loss(np.mean(losses))\n",
    "    if early_stopping.stop_training():\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('by', 13.958552360534668),\n",
       " ('and', 14.099173545837402),\n",
       " ('the', 14.320446014404297),\n",
       " ('in', 14.530234336853027),\n",
       " ('a', 14.558225631713867),\n",
       " ('of', 14.6000394821167),\n",
       " ('for', 14.89257526397705),\n",
       " ('to', 14.991289138793945),\n",
       " ('that', 15.217554092407227),\n",
       " ('was', 15.406394958496094),\n",
       " ('from', 15.501995086669922),\n",
       " ('he', 15.659364700317383),\n",
       " ('with', 15.713711738586426),\n",
       " ('under', 15.755976676940918),\n",
       " ('at', 15.783194541931152),\n",
       " ('their', 15.806936264038086),\n",
       " ('they', 15.893959045410156),\n",
       " ('were', 16.13153648376465),\n",
       " ('is', 16.161605834960938),\n",
       " ('on', 16.317657470703125),\n",
       " ('its', 16.333906173706055),\n",
       " ('have', 16.338603973388672),\n",
       " ('his', 16.423620223999023),\n",
       " ('than', 16.457183837890625),\n",
       " ('are', 16.563343048095703),\n",
       " ('would', 16.587234497070312),\n",
       " ('says', 16.665430068969727),\n",
       " ('doctor', 16.695606231689453),\n",
       " ('other', 16.72442054748535),\n",
       " ('since', 16.740633010864258),\n",
       " ('it', 16.76555633544922),\n",
       " ('i', 16.930435180664062),\n",
       " ('out', 16.932209014892578),\n",
       " ('warmth', 16.940357208251953),\n",
       " ('deadlock', 16.942188262939453),\n",
       " ('which', 16.951990127563477),\n",
       " ('last', 16.981977462768555),\n",
       " ('january', 16.993837356567383),\n",
       " ('few', 17.026473999023438),\n",
       " ('will', 17.0487117767334),\n",
       " ('as', 17.0638484954834),\n",
       " ('state', 17.088197708129883),\n",
       " ('belgian', 17.099050521850586),\n",
       " ('nassau', 17.117090225219727),\n",
       " ('each', 17.119462966918945),\n",
       " ('also', 17.126205444335938),\n",
       " ('two', 17.133657455444336),\n",
       " ('not', 17.149789810180664),\n",
       " ('new', 17.193376541137695),\n",
       " ('meals', 17.210620880126953),\n",
       " ('considered', 17.227985382080078),\n",
       " ('has', 17.250980377197266),\n",
       " ('or', 17.263090133666992),\n",
       " ('nonsense', 17.27228355407715),\n",
       " ('week', 17.284591674804688),\n",
       " ('world', 17.293174743652344),\n",
       " ('three', 17.307825088500977),\n",
       " ('fellini', 17.32098388671875),\n",
       " ('be', 17.32459259033203),\n",
       " ('year', 17.326984405517578),\n",
       " ('who', 17.331241607666016),\n",
       " ('simplicity', 17.34269905090332),\n",
       " ('wendell', 17.350866317749023),\n",
       " ('here', 17.3535099029541),\n",
       " ('when', 17.35464096069336),\n",
       " ('ike', 17.359832763671875),\n",
       " ('help', 17.366323471069336),\n",
       " ('effort', 17.396533966064453),\n",
       " ('way', 17.422143936157227),\n",
       " ('owners', 17.430072784423828),\n",
       " ('president', 17.43402099609375),\n",
       " ('but', 17.445545196533203),\n",
       " ('connections', 17.445941925048828),\n",
       " ('faced', 17.45806121826172),\n",
       " ('house', 17.459609985351562),\n",
       " ('this', 17.470800399780273),\n",
       " ('speed', 17.474071502685547),\n",
       " ('desk', 17.484853744506836),\n",
       " ('submitted', 17.4871768951416),\n",
       " ('governor', 17.490345001220703),\n",
       " ('gotten', 17.49859619140625),\n",
       " ('prove', 17.504362106323242),\n",
       " ('warden', 17.505048751831055),\n",
       " ('season', 17.50583839416504),\n",
       " ('masonic', 17.513853073120117),\n",
       " ('understandable', 17.519800186157227),\n",
       " ('quoted', 17.520095825195312),\n",
       " ('day', 17.52073097229004),\n",
       " ('choice', 17.525476455688477),\n",
       " ('bogey', 17.527204513549805),\n",
       " ('judge', 17.535160064697266),\n",
       " ('into', 17.544599533081055),\n",
       " ('been', 17.552719116210938),\n",
       " ('set', 17.56696891784668),\n",
       " ('so', 17.568113327026367),\n",
       " ('one', 17.568126678466797),\n",
       " ('dallas', 17.568849563598633),\n",
       " ('dedicated', 17.57192611694336),\n",
       " ('after', 17.57534408569336),\n",
       " ('dollars', 17.585460662841797)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_closest_word(word, topn=100):\n",
    "    word_distance = []\n",
    "    emb = net.embeddings_target\n",
    "    pdist = nn.PairwiseDistance()\n",
    "    i = word_to_index[word]\n",
    "    lookup_tensor_i = torch.tensor([i], dtype=torch.long)\n",
    "    v_i = emb(lookup_tensor_i)\n",
    "    for j in range(len(vocabulary)):\n",
    "        if j != i:\n",
    "            lookup_tensor_j = torch.tensor([j], dtype=torch.long)\n",
    "            v_j = emb(lookup_tensor_j)\n",
    "            word_distance.append((index_to_word[j], float(pdist(v_i, v_j))))\n",
    "    word_distance.sort(key=lambda x: x[1])\n",
    "    return word_distance[:topn]\n",
    "net=net.cpu()\n",
    "get_closest_word(\"county\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ood_ex",
   "language": "python",
   "name": "ood_ex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
